{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 29131 中有 9131 个触发bug\n",
      "67 83\n",
      "Comp_Model(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0.3, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (node_attention): NodeAttention(\n",
      "        (transform_edge): Linear(in_features=400, out_features=400, bias=True)\n",
      "        (tranform_node): Linear(in_features=400, out_features=400, bias=True)\n",
      "        (fc): Linear(in_features=800, out_features=1, bias=True)\n",
      "      )\n",
      "      (edge_attention): EdgeAttention(\n",
      "        (tranform_node): Linear(in_features=400, out_features=400, bias=True)\n",
      "        (tranform_edge): Linear(in_features=400, out_features=400, bias=True)\n",
      "        (attention_fc): Linear(in_features=800, out_features=2, bias=True)\n",
      "      )\n",
      "      (W_i): Linear(in_features=83, out_features=400, bias=True)\n",
      "      (W_i_nodes): Linear(in_features=67, out_features=400, bias=True)\n",
      "      (W_e): Linear(in_features=800, out_features=400, bias=True)\n",
      "      (W_h): Linear(in_features=400, out_features=400, bias=True)\n",
      "      (W_o): Linear(in_features=467, out_features=400, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=468, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from typing import Union\n",
    "from typing import Iterator\n",
    "from typing import Optional\n",
    "import re\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import ast\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import Random\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "import onnx\n",
    "import tvm\n",
    "from tvm.relay.frontend import onnx as ox\n",
    "from tvm import relay\n",
    "from tvm.autotvm.graph_tuner.utils import expr2graph\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import Adam, Optimizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "op_names = [\n",
    "\"torch.Linear\", \"core.Reshape\", \"core.ReplicatePad \", \"core.ReLU\", \n",
    "\"core.ExpandLast1\", \"core.Sub\", \"core.NCHWConv2d\", \"core.Tril\", \"core.Mul\", \n",
    "\"core.Clip\", \"core.Atan\", \"core.Squeeze\", \"core.ReduceMax\", \"core.Or\", \n",
    "\"core.NearestInterp\", \"core.ExpandLast4\", \"core.Conv1d\", \"core.Min\", \n",
    "\"None\", \"core.Max\", \"Concat \", \"core.Transpose\", \"core.Round\", \"core.ArgMax\", \n",
    "\"core.Where\", \"core.GELU\", \"core.AvgPool2d\", \"core.Ceil\", \"core.BatchNorm2d\", \n",
    "\"Constant\", \"core.LeakyReLU\", \"core.Sigmoid\", \"core.ReduceMean\", \"core.Add\", \n",
    "\"core.Neg\", \"core.Triu\", \"core.TrilinearInterp\", \"core.Floor\", \"core.ArgMin\", \n",
    "\"core.Div\", \"core.Xor\", \"core.Slice \", \"core.BilinearInterp\", \"core.LinearInterp\", \n",
    "\"core.Abs\", \"core.Equal\", \"core.ConstPad \", \"torch.Flatten\", \"core.And\", \"Input\", \n",
    "\"Cast \", \"torch.TorchReduceSum\", \"core.Cos\", \"core.ExpandLast2\", \"core.Softmax\", \n",
    "\"core.Sin\", \"core.MaxPool2d\", \"core.BicubicInterp\", \"core.Less\", \"core.PReLU\", \n",
    "\"core.ExpandLast3\", \"core.ReduceMin\", \"core.ReflectPad \", \"core.Tan\", \"core.Greater\"\n",
    "]\n",
    "\n",
    "data_type_names = [\n",
    "    \"i64\", \"i32\", \"b\", \"i8\", \"f64\", \"i16\", \"Unknow\", \"f32\", \"f16\", \"u8\"\n",
    "]\n",
    "\n",
    "vocab = {op:  idx for idx, op in enumerate(op_names)}\n",
    "op_vocab = {op: idx for idx, op in enumerate(op_names)}\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "json_filename = 'train.json'\n",
    "with open(json_filename, 'r') as jsonfile:\n",
    "    loaded_dataset_dicts = json.load(jsonfile)\n",
    "\n",
    "# Extract the lists from the loaded dataset\n",
    "node_info_set = [item['node_info'] for item in loaded_dataset_dicts]\n",
    "edge_info_set = [item['edge_info'] for item in loaded_dataset_dicts]\n",
    "graph_info_set = [item['graph_info'] for item in loaded_dataset_dicts]\n",
    "result_set = [item['result'] for item in loaded_dataset_dicts]\n",
    "count_result = 0\n",
    "count_true_result = 0\n",
    "for result in result_set:\n",
    "    if result == 1:\n",
    "        count_true_result += 1\n",
    "    count_result += 1\n",
    "print(\"在\",count_result,\"中有\",count_true_result,\"个触发bug\")\n",
    "\n",
    "def onek_encoding_unk(value, choices: List):\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def node_features(node_info:list):\n",
    "    features = onek_encoding_unk(node_info[1], op_names) \n",
    "    features.append(node_info[2])\n",
    "    return features\n",
    "\n",
    "def edge_features(edge_info:list):\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        features.append(edge_info[2][i])\n",
    "    tmp_list = onek_encoding_unk(edge_info[3], data_type_names)\n",
    "    for i in range(len(tmp_list)):\n",
    "        features.append(tmp_list[i])\n",
    "    return features\n",
    "\n",
    "# 初始化模型的参数\n",
    "def initialize_weights(model:nn.Module)->None:\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "def index_select_ND(source: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects the message features from source corresponding to the node or edge indices in index.\n",
    "    \"\"\"\n",
    "    index_size = index.size()  # (num_nodes/num_edges, max_num_edges)\n",
    "    suffix_dim = source.size()[1:]  # (hidden_size,) 也就是source中除了第一个维度之外的所有维度的大小\n",
    "    final_size = index_size + suffix_dim  # (num_nodes/num_edges, max_num_edges, hidden_size)\n",
    "    target = source.index_select(dim=0, index=index.view(-1))  # (num_nodes/num_edges * max_num_edges, hidden_size)\n",
    "    target = target.view(final_size)  # (num_nodes/num_edges, max_num_edges, hidden_size)\n",
    "    return target\n",
    "\n",
    "class TrainArgs:\n",
    "    no_cuda = False\n",
    "    gpu = 0\n",
    "    num_workers = 16\n",
    "    batch_size = 16\n",
    "    dataset_type = 'classification'\n",
    "    task_names = []\n",
    "    num_tasks = 1\n",
    "    seed = 0\n",
    "    hidden_size = 400\n",
    "    bias = False\n",
    "    depth = 3\n",
    "    dropout = 0.3\n",
    "    undirected = False\n",
    "    aggregation = 'norm'\n",
    "    aggregation_norm = 200\n",
    "    cuda = True\n",
    "    ffn_num_layers = 3\n",
    "    ffn_hidden_size = 300\n",
    "    init_lr = 1e-4\n",
    "    max_lr = 1e-3\n",
    "    final_lr = 1e-4\n",
    "    num_lrs = 1\n",
    "    warmup_epochs = 2.0\n",
    "    epochs = 100\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"The :code:`torch.device` on which to load and process data and models.\"\"\"\n",
    "        if not self.cuda:\n",
    "            return torch.device('cpu')\n",
    "\n",
    "        return torch.device('cuda', self.gpu)\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device: torch.device) -> None:\n",
    "        self.cuda = device.type == 'cuda'\n",
    "        self.gpu = device.index\n",
    "\n",
    "    @property\n",
    "    def cuda(self) -> bool:\n",
    "        \"\"\"Whether to use CUDA (i.e., GPUs) or not.\"\"\"\n",
    "        return not self.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    @cuda.setter\n",
    "    def cuda(self, cuda: bool) -> None:\n",
    "        self.no_cuda = not cuda\n",
    "        \n",
    "args = TrainArgs()\n",
    "\n",
    "node_info = node_info_set[0]\n",
    "edge_info = edge_info_set[0]\n",
    "result = result_set[0]\n",
    "\n",
    "NODE_FDIM = len(node_features(node_info[0]))\n",
    "EDGE_FDIM = len(edge_features(edge_info[0])) + NODE_FDIM\n",
    "print(NODE_FDIM, EDGE_FDIM)\n",
    "\n",
    "def get_node_fdim():\n",
    "    return NODE_FDIM\n",
    "\n",
    "def get_edge_fdim():\n",
    "    return EDGE_FDIM\n",
    "\n",
    "def graph_info_dim():\n",
    "    # 假设 graph_info 的前两个元素是数字\n",
    "    base_dim = 2\n",
    "    op_vector_dim = len(op_vocab) + 1  # 加1是因为有一个额外的类别表示未知的算子\n",
    "    # category_vector_dim = len(category_vocab) + 1  # 加1是因为有一个额外的类别表示未知的类别\n",
    "    return base_dim + op_vector_dim \n",
    "\n",
    "class CompGraphDatapoint:\n",
    "    def __init__(self,\n",
    "                 node_info: list,\n",
    "                 edge_info: list,\n",
    "                 graph_info: list,\n",
    "                 result: int,\n",
    "                 op_vocab: dict,\n",
    "                 ):\n",
    "        self.node_info = node_info\n",
    "        self.edge_info = edge_info\n",
    "        self.graph_info = self.encode_graph_info(graph_info, op_vocab)\n",
    "        self.result = result\n",
    "\n",
    "    def encode_graph_info(self, graph_info, op_vocab):\n",
    "        # 对算子进行编码\n",
    "        ops = graph_info[2]\n",
    "        op_indices = [op_vocab.get(op, len(op_vocab)) for op in ops]\n",
    "\n",
    "        # 对算子类别进行编码\n",
    "        # categories = graph_info[3]\n",
    "        # category_indices = [category_vocab.get(category, len(category_vocab)) for category in categories]\n",
    "\n",
    "        # 创建One-Hot编码向量\n",
    "        op_vector = [0] * (len(op_vocab) + 1)\n",
    "        for idx in op_indices:\n",
    "            op_vector[idx] = 1\n",
    "\n",
    "        # category_vector = [0] * (len(category_vocab) + 1)\n",
    "        # for idx in category_indices:\n",
    "        #     category_vector[idx] = 1\n",
    "\n",
    "        # 合并编码后的向量\n",
    "        encoded_graph_info = graph_info[:2] + op_vector \n",
    "        return encoded_graph_info\n",
    "\n",
    "class CompGraphDataset(Dataset):\n",
    "    def __init__(self, data:List[CompGraphDatapoint]):\n",
    "        self._data = data\n",
    "        self._scaler = None\n",
    "        self._batch_graph = []\n",
    "        self._random = Random()\n",
    "    \n",
    "    def AllCompGraphs(self):\n",
    "        return [(g.node_info, g.edge_info, g.result) for g in self._data]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self._data[item]\n",
    "    \n",
    "    def results(self):\n",
    "        return [d.result for d in self._data]\n",
    "    \n",
    "    def batch_graph(self):   \n",
    "        for d in self._data:\n",
    "            compgraph = CompGraph(d.node_info, d.edge_info)\n",
    "            self._batch_graph.append(compgraph)\n",
    "        return self._batch_graph\n",
    "\n",
    "class CompGraph:\n",
    "    def __init__(self, node_info:list, edge_info:list):\n",
    "        self.n_nodes = 0 # number of nodes\n",
    "        self.n_edges = 0 # number of edges\n",
    "        self.f_nodes = [] # mapping from node index to node features\n",
    "        self.f_edges = [] # mapping from edge index to concat(in_node, edge) features\n",
    "        self.a2b = [] # mapping from node index to incoming edge indices 这里是为了将输入到这个节点的边的信息汇聚过来\n",
    "        self.b2a = [] # mapping from edge index to the incoming node index\n",
    "\n",
    "        # self.node_id_to_index = {node_info[i][0]: i for i in range(len(node_info))}\n",
    "\n",
    "        # # get node features\n",
    "        # self.f_nodes = [node_features(node) for node in node_info]\n",
    "        # self.n_nodes = len(self.f_nodes)\n",
    "\n",
    "        # # initialize node to edge mapping for each ndoe\n",
    "        # for _ in range(self.n_nodes):\n",
    "        #     self.a2b.append([])\n",
    "\n",
    "        # for edge in edge_info:\n",
    "        #     f_edge = edge_features(edge)\n",
    "        #     # 确保 self.f_nodes[edge[0]] 是一个张量\n",
    "        #     # if isinstance(self.f_nodes[edge[0]], list):\n",
    "        #     #     self.f_nodes[edge[0]] = torch.tensor(self.f_nodes[edge[0]])\n",
    "\n",
    "        #     # # 确保 f_edge 是一个张量\n",
    "        #     # if isinstance(f_edge, list):\n",
    "        #     #     f_edge = torch.tensor(f_edge)\n",
    "        #     # self.f_edges.append(torch.cat([self.f_nodes[edge[0]], f_edge], dim=0)) # 这里将边的特征更新为原始的边特征与源点节点特征的拼接concat(in_node_info, edge_info)\n",
    "        #     self.f_edges.append(self.f_nodes[edge[0]] + f_edge)\n",
    "        #     self.a2b[edge[1]].append(edge[4])\n",
    "        #     self.b2a.append(edge[0])\n",
    "        # self.n_edges = len(self.f_edges)\n",
    "\n",
    "        # Build node features and initialize a2b mapping\n",
    "        self.f_nodes = [node_features(node) for node in node_info]\n",
    "        self.n_nodes = len(self.f_nodes)\n",
    "        for _ in range(self.n_nodes):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        # Create a mapping from old node identifiers to new indices\n",
    "        node_id_to_index = {node_info[i][0]: i for i in range(len(node_info))}\n",
    "\n",
    "        # Update edge_info to ensure source and target node identifiers match their index\n",
    "        for edge in edge_info:\n",
    "            source_node_id = edge[0]\n",
    "            target_node_id = edge[1]\n",
    "\n",
    "            # Update source and target node identifiers to match their indices\n",
    "            source_node_index = node_id_to_index[source_node_id]\n",
    "            target_node_index = node_id_to_index[target_node_id]\n",
    "            edge[0] = source_node_index\n",
    "            edge[1] = target_node_index\n",
    "\n",
    "            # Update f_edges with concatenated features\n",
    "            self.f_edges.append(self.f_nodes[source_node_index] + edge_features(edge))\n",
    "\n",
    "            # Update a2b mapping using target node index\n",
    "            self.a2b[target_node_index].append(edge[4])\n",
    "            self.b2a.append(source_node_index)\n",
    "\n",
    "        self.n_edges = len(self.f_edges)\n",
    "\n",
    "    def get_components(self):\n",
    "        return self.f_nodes, self.f_edges, self.a2b, self.b2a\n",
    "    \n",
    "class BatchCompGraph:\n",
    "    '''\n",
    "    处理一个batch的计算图，加速\n",
    "    '''\n",
    "    def __init__(self, comp_graphs:List[CompGraph]):\n",
    "        self.node_fdim = get_node_fdim()\n",
    "        self.edge_fdim = get_edge_fdim()\n",
    "        self.compgraph_count = len(comp_graphs)\n",
    "        # start n_nodes and n_edges at 1 b/c zero padding\n",
    "        self.n_nodes = 1\n",
    "        self.n_edges = 1\n",
    "        self.a_scope = []   # list of tuples indicating (start_node_index, num_nodes) for each graph\n",
    "        self.b_scope = []   # list of tuples indicating (start_edge_index, num_edges) for each graph\n",
    "\n",
    "        # all start with zero padding so that indexing with zero padding returns zeros\n",
    "        f_nodes = [[0] * self.node_fdim]  # node features\n",
    "        f_edges = [[0] * self.edge_fdim]  # combined node/edge features\n",
    "        a2b = [[]]  # mapping from node index to incoming edge indices\n",
    "        b2a = [0]  # mapping from edge index to the index of the node the edge is coming from\n",
    "\n",
    "        for comp_graph in comp_graphs:\n",
    "            f_nodes.extend(comp_graph.f_nodes)\n",
    "            f_edges.extend(comp_graph.f_edges)\n",
    "\n",
    "            for a in range(comp_graph.n_nodes):\n",
    "                a2b.append([b + self.n_edges for b in comp_graph.a2b[a]])\n",
    "\n",
    "            for b in range(comp_graph.n_edges):\n",
    "                b2a.append(self.n_nodes + comp_graph.b2a[b])\n",
    "\n",
    "            self.a_scope.append((self.n_nodes, comp_graph.n_nodes))\n",
    "            self.b_scope.append((self.n_edges, comp_graph.n_edges))\n",
    "            self.n_nodes += comp_graph.n_nodes\n",
    "            self.n_edges += comp_graph.n_edges\n",
    "\n",
    "        self.max_num_edges = max(1, max(len(in_bonds) for in_bonds in a2b))  # max with 1 to fix a crash in case of\n",
    "        # all single-heavy-node graph\n",
    "\n",
    "        self.f_nodes = torch.FloatTensor(f_nodes)\n",
    "        self.f_edges = torch.FloatTensor(f_edges)\n",
    "        self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_edges - len(a2b[a])) for a in range(self.n_nodes)])\n",
    "        self.b2a = torch.LongTensor(b2a)\n",
    "        self.b2b = None  # try to avoid computing b2b b/c O(n_nodes^3)\n",
    "        self.a2a = None  # only needed if using node messages\n",
    "\n",
    "    def get_components(self):\n",
    "        return self.f_nodes, self.f_edges, self.a2b, self.b2a, self.a_scope, self.b_scope\n",
    "    \n",
    "class NodeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NodeAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.transform_edge = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tranform_node = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "\n",
    "    def forward(self, h_self, h_neigh):\n",
    "        # h_self: [num_nodes, 1, hidden_size]\n",
    "        # h_neigh: [num_nodes, num_max_edges, hidden_size]\n",
    "        # 将自身状态复制到每个边的数量\n",
    "        h_self = self.tranform_node(h_self)\n",
    "        h_neigh = self.transform_edge(h_neigh)\n",
    "        h_self_expanded = h_self.unsqueeze(1).expand(-1, h_neigh.size(1), -1)  # [num_nodes, num_max_edges, hidden_size]\n",
    "        # 将自身状态和邻边状态合并\n",
    "        combined_h = torch.cat([h_self_expanded, h_neigh], dim=-1)  # [num_nodes, num_max_edges, 2 * hidden_size]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = self.fc(combined_h.view(-1, 2 * self.hidden_size)).view(h_self.size(0), h_neigh.size(1))  # [num_nodes, num_max_edges]\n",
    "        \n",
    "        # 添加自身对自身的注意力\n",
    "        self_attention = self.fc(torch.cat([h_self, h_self], dim=-1)).view(h_self.size(0), 1)  # [num_nodes, 1]\n",
    "        scores = torch.cat([self_attention, scores], dim=1)  # [num_nodes, num_max_edges + 1]\n",
    "        \n",
    "        return torch.softmax(scores, dim=1)  # [num_nodes, num_max_edges + 1]\n",
    "\n",
    "class EdgeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EdgeAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tranform_node = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tranform_edge = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_fc = nn.Linear(2 * hidden_size, 2)  # 输出两个权重：w1 和 w2\n",
    "\n",
    "    def forward(self, node_state, edge_state):\n",
    "        # node_state: [num_edges, hidden_size]\n",
    "        # edge_state: [num_edges, hidden_size]\n",
    "        node_state = self.tranform_node(node_state)\n",
    "        edge_state = self.tranform_edge(edge_state)\n",
    "        # 将节点状态和边状态拼接\n",
    "        combined_state = torch.cat([node_state, edge_state], dim=-1)  # [num_edges, 2 * hidden_size]\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        weights = self.attention_fc(combined_state)  # [num_edges, 2]\n",
    "        weights = torch.softmax(weights, dim=1)  # 归一化权重\n",
    "        \n",
    "        # 计算新的边状态\n",
    "        new_edge_state = weights[:, 0].unsqueeze(1) * node_state + weights[:, 1].unsqueeze(1) * edge_state  # [num_edges, hidden_size]\n",
    "        \n",
    "        return new_edge_state\n",
    "    \n",
    "class NodeAttention_multihead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4):\n",
    "        super(NodeAttention_multihead, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 为每个头创建独立的变换矩阵\n",
    "        self.transform_edge = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_heads)])  \n",
    "        self.transform_node = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_heads)])  \n",
    "        self.fc = nn.ModuleList([nn.Linear(2 * hidden_size, 1) for _ in range(num_heads)])  \n",
    "\n",
    "    def forward(self, h_self, h_neigh):\n",
    "        # h_self: [num_nodes, hidden_size]\n",
    "        # h_neigh: [num_nodes, num_max_edges, hidden_size]\n",
    "\n",
    "        head_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            # 对节点和邻边应用独立变换\n",
    "            transformed_self = self.transform_node[i](h_self)  # [num_nodes, hidden_size]\n",
    "            transformed_neigh = self.transform_edge[i](h_neigh)  # [num_nodes, num_max_edges, hidden_size]\n",
    "\n",
    "            # 将自身状态复制到每个边的数量，然后和邻边状态合并\n",
    "            combined_h = torch.cat([\n",
    "                transformed_self.unsqueeze(1).expand(-1, h_neigh.size(1), -1),  # [num_nodes, num_max_edges, hidden_size]\n",
    "                transformed_neigh  # [num_nodes, num_max_edges, hidden_size]\n",
    "            ], dim=-1)  # [num_nodes, num_max_edges, 2 * hidden_size]\n",
    "\n",
    "            # 计算注意力分数\n",
    "            scores = self.fc[i](combined_h.view(-1, 2 * self.hidden_size)).view(h_self.size(0), h_neigh.size(1))  # [num_nodes, num_max_edges]\n",
    "            # 添加自身对自身的注意力\n",
    "            self_attention = self.fc[i](torch.cat([h_self, h_self], dim=-1)).view(h_self.size(0), 1)  # [num_nodes, 1]\n",
    "            scores = torch.cat([self_attention, scores], dim=1)  # [num_nodes, num_max_edges + 1]\n",
    "            head_outputs.append(scores)\n",
    "\n",
    "        # 将所有头的输出进行平均\n",
    "        scores = torch.stack(head_outputs, dim=2).mean(dim=2)  # [num_nodes, num_max_edges]\n",
    "        return torch.softmax(scores, dim=1)  # [num_nodes, num_max_edges]\n",
    "\n",
    "class EdgeAttention_multihead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=4):\n",
    "        super(EdgeAttention_multihead, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.transform_node = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_heads)])\n",
    "        self.transform_edge = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_heads)])\n",
    "        self.attention_fc = nn.ModuleList([nn.Linear(2 * hidden_size, 2) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, node_state, edge_state):\n",
    "        # node_state: [num_edges, hidden_size]\n",
    "        # edge_state: [num_edges, hidden_size]\n",
    "\n",
    "        head_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            transformed_node = self.transform_node[i](node_state)  # [num_edges, hidden_size]\n",
    "            transformed_edge = self.transform_edge[i](edge_state)  # [num_edges, hidden_size]\n",
    "\n",
    "            # 将节点状态和边状态拼接\n",
    "            combined_state = torch.cat([transformed_node, transformed_edge], dim=-1)  # [num_edges, 2 * hidden_size]\n",
    "\n",
    "            # 计算注意力权重\n",
    "            weights = self.attention_fc[i](combined_state)  # [num_edges, 2]\n",
    "            weights = torch.softmax(weights, dim=1)  # [num_edges, 2]\n",
    "\n",
    "            # 根据权重计算新的边状态\n",
    "            new_edge_state = weights[:, 0].unsqueeze(1) * transformed_node + weights[:, 1].unsqueeze(1) * transformed_edge  # [num_edges, hidden_size]\n",
    "            head_outputs.append(new_edge_state)\n",
    "\n",
    "        # 将所有头的输出进行平均\n",
    "        new_edge_state = torch.stack(head_outputs, dim=2).mean(dim=2)  # [num_edges, hidden_size]\n",
    "        return new_edge_state\n",
    "\n",
    "\n",
    "# MPNEncodr类，负责计算图的编码过程，首先将每条边转换为一个隐藏状态，然后通过多次message_passing来更新这些隐藏状态\n",
    "# 用来模型信息在图中的传递\n",
    "'''\n",
    "要训练的权重参数如下：\n",
    "W_i: 用于初始边特征到隐藏状态的转换\n",
    "W_h: 在每次message passing迭代中用于更新隐藏状态的权重\n",
    "W_o: 将最终的节点表示和原始的节点表示拼接后，用于生成最终的原子隐藏状态\n",
    "'''\n",
    "# 消息传递的逻辑：边的特征是原始边特征与源点特征的拼接，每一次传递过程中，边的特征会传递给终点节点\n",
    "class MPNEncoder(nn.Module):\n",
    "    def __init__(self, args, node_fdim, edge_fdim):\n",
    "        super(MPNEncoder, self).__init__()\n",
    "        self.node_fdim = node_fdim\n",
    "        self.edge_fdim = edge_fdim  # 这里的edge_fdim是原始的edge_fdim + node_fdim\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.bias = args.bias\n",
    "        self.depth = args.depth\n",
    "        self.dropout = args.dropout\n",
    "        self.device = args.device\n",
    "        self.aggregation = args.aggregation\n",
    "        self.aggregation_norm = args.aggregation_norm\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        self.act_func = nn.ReLU()\n",
    "        self.cached_zero_vector = nn.Parameter(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "\n",
    "        self.node_attention = NodeAttention(self.hidden_size)\n",
    "        self.edge_attention = EdgeAttention(self.hidden_size)\n",
    "\n",
    "        # Input\n",
    "        input_dim = self.edge_fdim\n",
    "        self.W_i = nn.Linear(input_dim, self.hidden_size)\n",
    "        w_h_input_size = self.hidden_size\n",
    "        self.W_i_nodes = nn.Linear(self.node_fdim, self.hidden_size)\n",
    "\n",
    "        # Shared weight matrix across depths (default)\n",
    "        self.W_e = nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "        self.W_h = nn.Linear(w_h_input_size, self.hidden_size)\n",
    "        self.W_o = nn.Linear(self.node_fdim + self.hidden_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, batch_graph, encoded_graph_infos):\n",
    "        f_nodes, f_edges, a2b, b2a, a_scope, b_scope = batch_graph.get_components()\n",
    "        f_nodes, f_edges, a2b, b2a, = f_nodes.to(self.device), f_edges.to(self.device), a2b.to(self.device), b2a.to(self.device)\n",
    "        self.W_i = self.W_i.to(self.device)\n",
    "        self.W_h = self.W_h.to(self.device)\n",
    "        self.W_o = self.W_o.to(self.device)\n",
    "\n",
    "        # Initial transformations\n",
    "        edge_message = self.act_func(self.W_i(f_edges))  # Transform edge features    num_edges * hidden_size\n",
    "        nodes_message = self.act_func(self.W_i_nodes(f_nodes))  # Transform node features   num_nodes * hidden_size\n",
    "\n",
    "        # Message passing\n",
    "        for depth in range(self.depth):\n",
    "            # Gather messages from edges to nodes\n",
    "            nei_a_message = index_select_ND(edge_message, a2b)  # [num_nodes, num_max_edges, hidden_size]\n",
    "            \n",
    "            # 计算注意力分数，包括节点自身状态\n",
    "            attention_scores = self.node_attention(nodes_message, nei_a_message)  # [num_nodes, num_max_edges + 1]\n",
    "            # 使用注意力分数加权邻边消息\n",
    "            weighted_messages = attention_scores[:, 1:].unsqueeze(-1) * nei_a_message  # [num_nodes, num_max_edges, hidden_size]\n",
    "            aggregated_message = weighted_messages.sum(dim=1)  # [num_nodes, hidden_size]\n",
    "            \n",
    "            # 加上自身消息的权重\n",
    "            self_message = attention_scores[:, 0].unsqueeze(1) * nodes_message  # [num_nodes, hidden_size]\n",
    "            nodes_message = self.act_func(self.W_h(self_message + aggregated_message))  # [num_nodes, hidden_size]\n",
    "\n",
    "            src_node_states = nodes_message[b2a]\n",
    "            edge_message = self.edge_attention(src_node_states, edge_message)\n",
    "\n",
    "\n",
    "        # Prepare output\n",
    "        final_node_representation = torch.cat([f_nodes, nodes_message], dim=1)\n",
    "        node_hiddens = self.act_func(self.W_o(final_node_representation))\n",
    "        node_hiddens = self.dropout_layer(node_hiddens)\n",
    "        \n",
    "        # readout\n",
    "        graph_vecs = []\n",
    "        for i, (a_start, a_size) in enumerate(a_scope):\n",
    "            if a_size == 0:\n",
    "                graph_vecs.append(self.cached_zero_vector)\n",
    "            else:\n",
    "                # print('graph_vecs_size: ', len(graph_vecs))\n",
    "                cur_hiddens = node_hiddens.narrow(0, a_start, a_size)\n",
    "                # print('cur_hiddens_size: ', cur_hiddens.size())\n",
    "                graph_vector = cur_hiddens\n",
    "                if self.aggregation == 'mean':\n",
    "                    graph_vector = graph_vector.sum(dim = 0) / a_size\n",
    "                elif self.aggregation == 'sum':\n",
    "                    graph_vector = graph_vector.sum(dim = 0)\n",
    "                elif self.aggregation == 'norm':\n",
    "                    graph_vector = graph_vector.sum(dim = 0) / self.aggregation_norm\n",
    "            graph_vecs.append(graph_vector)\n",
    "        graph_vecs = torch.stack(graph_vecs, dim = 0)\n",
    "        encoded_graph_infos_tensor = torch.tensor(encoded_graph_infos, dtype=torch.float).to(self.device)\n",
    "        graph_vecs = torch.cat([graph_vecs, encoded_graph_infos_tensor], dim=1)\n",
    "        return graph_vecs\n",
    "\n",
    "class MPN(nn.Module):\n",
    "    def __init__(self, args, node_fdim=None, edge_fdim=None):\n",
    "        super(MPN, self).__init__()\n",
    "        self.node_fdim = node_fdim or get_node_fdim()\n",
    "        self.edge_fdim = edge_fdim or get_edge_fdim()\n",
    "        self.device = args.device\n",
    "        self.encoder = MPNEncoder(args, self.node_fdim, self.edge_fdim)\n",
    "\n",
    "    def forward(self, batch_comp_graph, encoded_graph_infos):\n",
    "        if type(batch_comp_graph) != BatchCompGraph:\n",
    "            batch_comp_graph = BatchCompGraph(batch_comp_graph)\n",
    "        return self.encoder(batch_comp_graph, encoded_graph_infos)\n",
    "    \n",
    "class Comp_Model(nn.Module):\n",
    "    def __init__(self, args, graph_info_dim):\n",
    "        super(Comp_Model, self).__init__()\n",
    "        self.classification = args.dataset_type == 'classification'\n",
    "        self.output_size = args.num_tasks\n",
    "        if self.classification:\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        self.create_encoder(args)\n",
    "        self.create_ffn(args, graph_info_dim)\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def create_encoder(self, args):\n",
    "        self.encoder = MPN(args)\n",
    "\n",
    "    def create_ffn(self, args, graph_info_dim):\n",
    "        first_linear_dim = args.hidden_size + graph_info_dim  # 加上 encoded_graph_info 的长度\n",
    "        dropout = nn.Dropout(args.dropout)\n",
    "        activation = nn.ReLU()\n",
    "        ffn = [dropout, nn.Linear(first_linear_dim, args.ffn_hidden_size)]\n",
    "        for _ in range(args.ffn_num_layers - 2):\n",
    "            ffn.extend([activation, dropout, nn.Linear(args.ffn_hidden_size, args.ffn_hidden_size)])\n",
    "        ffn.extend([activation, dropout, nn.Linear(args.ffn_hidden_size, self.output_size)])\n",
    "        self.ffn = nn.Sequential(*ffn)\n",
    "\n",
    "    def forward(self, batch, encoded_graph_infos):\n",
    "        output = self.ffn(self.encoder(batch, encoded_graph_infos))\n",
    "        if self.classification:\n",
    "            output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Comp_Model(args, graph_info_dim())\n",
    "model = model.to(args.device)\n",
    "print(model)\n",
    "\n",
    "class NoamLR(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Noam learning rate scheduler with piecewise linear increase and exponential decay.\n",
    "\n",
    "    The learning rate increases linearly from init_lr to max_lr over the course of\n",
    "    the first warmup_steps (where :code:`warmup_steps = warmup_epochs * steps_per_epoch`).\n",
    "    Then the learning rate decreases exponentially from :code:`max_lr` to :code:`final_lr` over the\n",
    "    course of the remaining :code:`total_steps - warmup_steps` (where :code:`total_steps =\n",
    "    total_epochs * steps_per_epoch`). This is roughly based on the learning rate\n",
    "    schedule from `Attention is All You Need <https://arxiv.org/abs/1706.03762>`_, section 5.3.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 optimizer: Optimizer,\n",
    "                 warmup_epochs: List[Union[float, int]],\n",
    "                 total_epochs: List[int],\n",
    "                 steps_per_epoch: int,\n",
    "                 init_lr: List[float],\n",
    "                 max_lr: List[float],\n",
    "                 final_lr: List[float]):\n",
    "\n",
    "        assert len(optimizer.param_groups) == len(warmup_epochs) == len(total_epochs) == len(init_lr) == \\\n",
    "               len(max_lr) == len(final_lr)\n",
    "\n",
    "        self.num_lrs = len(optimizer.param_groups)\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = np.array(warmup_epochs)\n",
    "        self.total_epochs = np.array(total_epochs)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.init_lr = np.array(init_lr)\n",
    "        self.max_lr = np.array(max_lr)\n",
    "        self.final_lr = np.array(final_lr)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.lr = init_lr\n",
    "        self.warmup_steps = (self.warmup_epochs * self.steps_per_epoch).astype(int)\n",
    "        self.total_steps = self.total_epochs * self.steps_per_epoch\n",
    "        self.linear_increment = (self.max_lr - self.init_lr) / self.warmup_steps\n",
    "\n",
    "        self.exponential_gamma = (self.final_lr / self.max_lr) ** (1 / (self.total_steps - self.warmup_steps))\n",
    "\n",
    "        super(NoamLR, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        return list(self.lr)\n",
    "\n",
    "    def step(self, current_step: int = None):\n",
    "        if current_step is not None:\n",
    "            self.current_step = current_step\n",
    "        else:\n",
    "            self.current_step += 1\n",
    "\n",
    "        for i in range(self.num_lrs):\n",
    "            if self.current_step <= self.warmup_steps[i]:\n",
    "                self.lr[i] = self.init_lr[i] + self.current_step * self.linear_increment[i]\n",
    "            elif self.current_step <= self.total_steps[i]:\n",
    "                self.lr[i] = self.max_lr[i] * (self.exponential_gamma[i] ** (self.current_step - self.warmup_steps[i]))\n",
    "            else:  # theoretically this case should never be reached since training should stop at total_steps\n",
    "                self.lr[i] = self.final_lr[i]\n",
    "\n",
    "            self.optimizer.param_groups[i]['lr'] = self.lr[i]\n",
    "\n",
    "def construct_compgraph_batch(data):\n",
    "    data = CompGraphDataset(data)\n",
    "    # return data.batch_graph()  # Forces computation and caching of the BatchCompGraph for the CompGraphs\n",
    "    return data\n",
    "\n",
    "class CompGraphSampler(Sampler):\n",
    "    def __init__(self, dataset, shuffle=False, seed=0):\n",
    "        super(Sampler, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self._random = Random(seed)\n",
    "        self.positive_indices = self.negative_indices = None\n",
    "        self.length = len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            self._random.shuffle(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class CompGraphDataLoader(DataLoader):\n",
    "    def __init__(self,\n",
    "                 dataset: CompGraphDataset,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 8,\n",
    "                 shuffle: bool = False,\n",
    "                 seed: int = 0):\n",
    "\n",
    "        self._dataset = dataset\n",
    "        self._batch_size = batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._shuffle = shuffle\n",
    "        self._seed = seed\n",
    "        self._context = None\n",
    "        self._class_balance = False\n",
    "        self._timeout = 0\n",
    "        is_main_thread = threading.current_thread() is threading.main_thread()\n",
    "        \n",
    "        if not is_main_thread and self._num_workers > 0:\n",
    "            self._context = 'forkserver'  # In order to prevent a hanging\n",
    "            self._timeout = 3600  # Just for sure that the DataLoader won't hang\n",
    "\n",
    "        self._sampler = CompGraphSampler(\n",
    "            dataset=self._dataset,\n",
    "            shuffle=self._shuffle,\n",
    "            seed=self._seed\n",
    "        )\n",
    "\n",
    "        super(CompGraphDataLoader, self).__init__(\n",
    "            dataset=self._dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            sampler=self._sampler,\n",
    "            num_workers=self._num_workers,\n",
    "            collate_fn=construct_compgraph_batch,\n",
    "            multiprocessing_context=self._context,\n",
    "            timeout=self._timeout\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> List[List[Optional[float]]]:\n",
    "        if self._class_balance or self._shuffle:\n",
    "            raise ValueError('Cannot safely extract targets when class balance or shuffle are enabled.')\n",
    "\n",
    "        return [self._dataset[index].targets for index in self._sampler]\n",
    "\n",
    "    @property\n",
    "    def iter_size(self) -> int:\n",
    "        return len(self._sampler)\n",
    "\n",
    "    def __iter__(self) -> Iterator[CompGraphDataset]:\n",
    "        return super(CompGraphDataLoader, self).__iter__()\n",
    "    \n",
    "\n",
    "data = CompGraphDataset([\n",
    "    CompGraphDatapoint(\n",
    "        node_info=node_info_set[i],\n",
    "        edge_info=edge_info_set[i],\n",
    "        graph_info=graph_info_set[i],\n",
    "        result=result_set[i],\n",
    "        op_vocab=op_vocab\n",
    "    ) for i in range(len(node_info_set))\n",
    "])\n",
    "\n",
    "random = Random()\n",
    "sizes = [0.8, 0.1, 0.1]\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int(sizes[0] * len(data))\n",
    "train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "train_source = [data[i] for i in indices[:train_size]]\n",
    "val = [data[i] for i in indices[train_size:train_val_size]]\n",
    "test = [data[i] for i in indices[train_val_size:]]\n",
    "\n",
    "train_data = CompGraphDataset(train_source)\n",
    "val_data = CompGraphDataset(val)\n",
    "test_data = CompGraphDataset(test)\n",
    "\n",
    "train_data_loader = CompGraphDataLoader(\n",
    "    dataset = train_data,\n",
    "    batch_size = args.batch_size,\n",
    "    num_workers= 8,\n",
    "    shuffle = True,\n",
    "    seed = args.seed\n",
    ")\n",
    "\n",
    "val_data_loader = CompGraphDataLoader(\n",
    "    dataset = val_data,\n",
    "    batch_size = args.batch_size,\n",
    "    num_workers= 8\n",
    ")\n",
    "\n",
    "test_data_loader = CompGraphDataLoader(\n",
    "    dataset = test_data,\n",
    "    batch_size = args.batch_size,\n",
    "    num_workers= 8,\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "params = [{'params': model.parameters(), 'lr': args.init_lr, 'weight_decay': 0}]\n",
    "optimizer = Adam(params)\n",
    "metric_func = mean_squared_error\n",
    "\n",
    "# scheduler\n",
    "scheduler = NoamLR(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=[args.warmup_epochs],\n",
    "    total_epochs=[args.epochs] * args.num_lrs,\n",
    "    steps_per_epoch=len(train_data) // args.batch_size,\n",
    "    init_lr=[args.init_lr],\n",
    "    max_lr=[args.max_lr],\n",
    "    final_lr=[args.final_lr]\n",
    ")\n",
    "\n",
    "def train(model, train_data_loader, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_data_loader:\n",
    "        batch_graphs = batch.batch_graph()\n",
    "        encoded_graph_infos = [datapoint.graph_info for datapoint in batch]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_graphs, encoded_graph_infos)\n",
    "        targets = torch.tensor(batch.results(), dtype=torch.float).to(args.device)\n",
    "        loss = F.binary_cross_entropy(output.squeeze(1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, args, threshold=0.3):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    correct_predictions_positives = 0\n",
    "    total_samples = 0\n",
    "    total_positives = 0\n",
    "    predicted_positives = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_graphs = batch.batch_graph()\n",
    "            encoded_graph_infos = [datapoint.graph_info for datapoint in batch]\n",
    "            output = model(batch_graphs, encoded_graph_infos)\n",
    "            targets = torch.tensor(batch.results(), dtype=torch.float).to(args.device)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(1), targets)\n",
    "            total_loss += loss.item()\n",
    "            predictions = (output.squeeze(1) >= threshold).long()\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            correct_predictions_positives += ((predictions == 1) & (targets == 1)).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "            total_positives += (targets == 1).sum().item()\n",
    "            predicted_positives += (predictions == 1).sum().item()\n",
    "    overall_accuracy = correct_predictions / total_samples * 100 if total_samples > 0 else 0\n",
    "    positive_accuracy = correct_predictions_positives / total_positives * 100 if total_positives > 0 else 0\n",
    "    return total_loss / len(data_loader), overall_accuracy, positive_accuracy, total_positives, predicted_positives\n",
    "\n",
    "def main(args):\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        train_loss = train(model, train_data_loader, optimizer, scheduler, args)\n",
    "        val_loss, val_accuracy, val_positive_accuracy, total_positives, predicted_positives = evaluate(model, val_data_loader, args)\n",
    "        print(f'Epoch {epoch+1}/{args.epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, '\n",
    "              f'Positive Accuracy: {val_positive_accuracy:.2f}%, Total Positives: {total_positives}, Predicted Positives: {predicted_positives}')\n",
    "\n",
    "        # Check if the current model is better than the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            # Save the best model state\n",
    "            torch.save(best_model_state, 'best_model.pth')\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_loss, test_accuracy, test_positive_accuracy, test_total_positives, test_predicted_positives = evaluate(model, test_data_loader, args)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, '\n",
    "          f'Test Positive Accuracy: {test_positive_accuracy:.2f}%, Test Total Positives: {test_total_positives}, Test Predicted Positives: {test_predicted_positives}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_info': [[4, 'Input', 0],\n",
       "  [5, 'core.Clip', 1],\n",
       "  [2, 'Concat ', 3],\n",
       "  [1, 'Cast ', 1],\n",
       "  [3, 'core.ReduceMax', 1]],\n",
       " 'edge_info': [[4, 5, [19, 14, 10, 1, 0], 'i64', 0],\n",
       "  [5, 2, [19, 14, 10, 1, 0], 'i64', 1],\n",
       "  [5, 2, [19, 14, 10, 1, 0], 'i64', 2],\n",
       "  [5, 2, [19, 14, 10, 1, 0], 'i64', 3],\n",
       "  [5, 1, [19, 14, 10, 1, 0], 'i64', 4],\n",
       "  [1, 3, [19, 14, 10, 1, 0], 'f64', 5]],\n",
       " 'graph_info': [5,\n",
       "  6,\n",
       "  ['Input', 'Cast ', 'core.ReduceMax', 'Concat ', 'core.Clip']],\n",
       " 'result': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.7058, Val Loss: 0.5745, Val Accuracy: 67.32%, Positive Accuracy: 89.14%, Total Positives: 921, Predicted Positives: 1673\n",
      "Epoch 2/100, Train Loss: 0.5396, Val Loss: 0.4776, Val Accuracy: 77.07%, Positive Accuracy: 76.76%, Total Positives: 921, Predicted Positives: 1161\n",
      "Epoch 3/100, Train Loss: 0.4569, Val Loss: 0.4741, Val Accuracy: 82.18%, Positive Accuracy: 72.53%, Total Positives: 921, Predicted Positives: 934\n",
      "Epoch 4/100, Train Loss: 0.4164, Val Loss: 0.3808, Val Accuracy: 81.53%, Positive Accuracy: 81.87%, Total Positives: 921, Predicted Positives: 1125\n",
      "Epoch 5/100, Train Loss: 0.3884, Val Loss: 0.3667, Val Accuracy: 82.22%, Positive Accuracy: 81.54%, Total Positives: 921, Predicted Positives: 1099\n",
      "Epoch 6/100, Train Loss: 0.3783, Val Loss: 0.3564, Val Accuracy: 85.34%, Positive Accuracy: 74.92%, Total Positives: 921, Predicted Positives: 886\n",
      "Epoch 7/100, Train Loss: 0.3505, Val Loss: 0.3488, Val Accuracy: 83.01%, Positive Accuracy: 81.43%, Total Positives: 921, Predicted Positives: 1074\n",
      "Epoch 8/100, Train Loss: 0.3425, Val Loss: 0.3423, Val Accuracy: 84.28%, Positive Accuracy: 76.55%, Total Positives: 921, Predicted Positives: 947\n",
      "Epoch 9/100, Train Loss: 0.3386, Val Loss: 0.3505, Val Accuracy: 85.48%, Positive Accuracy: 72.96%, Total Positives: 921, Predicted Positives: 846\n",
      "Epoch 10/100, Train Loss: 0.3526, Val Loss: 0.3401, Val Accuracy: 85.07%, Positive Accuracy: 77.20%, Total Positives: 921, Predicted Positives: 936\n",
      "Epoch 11/100, Train Loss: 0.3317, Val Loss: 0.3320, Val Accuracy: 84.65%, Positive Accuracy: 81.43%, Total Positives: 921, Predicted Positives: 1026\n",
      "Epoch 12/100, Train Loss: 0.3236, Val Loss: 0.3279, Val Accuracy: 85.93%, Positive Accuracy: 76.55%, Total Positives: 921, Predicted Positives: 899\n",
      "Epoch 13/100, Train Loss: 0.3183, Val Loss: 0.3209, Val Accuracy: 86.34%, Positive Accuracy: 76.76%, Total Positives: 921, Predicted Positives: 891\n",
      "Epoch 14/100, Train Loss: 0.3266, Val Loss: 0.3148, Val Accuracy: 87.30%, Positive Accuracy: 76.87%, Total Positives: 921, Predicted Positives: 865\n",
      "Epoch 15/100, Train Loss: 0.3160, Val Loss: 0.3112, Val Accuracy: 86.82%, Positive Accuracy: 77.96%, Total Positives: 921, Predicted Positives: 899\n",
      "Epoch 16/100, Train Loss: 0.3054, Val Loss: 0.3111, Val Accuracy: 87.20%, Positive Accuracy: 79.15%, Total Positives: 921, Predicted Positives: 910\n",
      "Epoch 17/100, Train Loss: 0.3021, Val Loss: 0.3121, Val Accuracy: 87.13%, Positive Accuracy: 77.52%, Total Positives: 921, Predicted Positives: 882\n",
      "Epoch 18/100, Train Loss: 0.3057, Val Loss: 0.4102, Val Accuracy: 84.93%, Positive Accuracy: 74.48%, Total Positives: 921, Predicted Positives: 890\n",
      "Epoch 19/100, Train Loss: 0.3180, Val Loss: 0.3111, Val Accuracy: 86.78%, Positive Accuracy: 78.07%, Total Positives: 921, Predicted Positives: 902\n",
      "Epoch 20/100, Train Loss: 0.2942, Val Loss: 0.3083, Val Accuracy: 87.23%, Positive Accuracy: 78.83%, Total Positives: 921, Predicted Positives: 903\n",
      "Epoch 21/100, Train Loss: 0.2922, Val Loss: 0.3419, Val Accuracy: 87.40%, Positive Accuracy: 78.07%, Total Positives: 921, Predicted Positives: 884\n",
      "Epoch 22/100, Train Loss: 0.3126, Val Loss: 0.3537, Val Accuracy: 86.96%, Positive Accuracy: 78.61%, Total Positives: 921, Predicted Positives: 907\n",
      "Epoch 23/100, Train Loss: 0.2921, Val Loss: 0.3333, Val Accuracy: 87.33%, Positive Accuracy: 80.46%, Total Positives: 921, Predicted Positives: 930\n",
      "Epoch 24/100, Train Loss: 0.2940, Val Loss: 0.3009, Val Accuracy: 86.92%, Positive Accuracy: 81.87%, Total Positives: 921, Predicted Positives: 968\n",
      "Epoch 25/100, Train Loss: 0.2846, Val Loss: 0.3380, Val Accuracy: 88.33%, Positive Accuracy: 77.85%, Total Positives: 921, Predicted Positives: 853\n",
      "Epoch 26/100, Train Loss: 0.3387, Val Loss: 0.2841, Val Accuracy: 89.12%, Positive Accuracy: 77.85%, Total Positives: 921, Predicted Positives: 830\n",
      "Epoch 27/100, Train Loss: 0.2836, Val Loss: 0.2887, Val Accuracy: 88.02%, Positive Accuracy: 80.89%, Total Positives: 921, Predicted Positives: 918\n",
      "Epoch 28/100, Train Loss: 0.2849, Val Loss: 0.2576, Val Accuracy: 89.67%, Positive Accuracy: 82.08%, Total Positives: 921, Predicted Positives: 892\n",
      "Epoch 29/100, Train Loss: 0.2641, Val Loss: 0.2746, Val Accuracy: 89.22%, Positive Accuracy: 83.17%, Total Positives: 921, Predicted Positives: 925\n",
      "Epoch 30/100, Train Loss: 0.2527, Val Loss: 0.2647, Val Accuracy: 90.42%, Positive Accuracy: 79.15%, Total Positives: 921, Predicted Positives: 816\n",
      "Epoch 31/100, Train Loss: 0.2662, Val Loss: 0.2535, Val Accuracy: 90.39%, Positive Accuracy: 81.54%, Total Positives: 921, Predicted Positives: 861\n",
      "Epoch 32/100, Train Loss: 0.2651, Val Loss: 0.2571, Val Accuracy: 89.53%, Positive Accuracy: 81.65%, Total Positives: 921, Predicted Positives: 888\n",
      "Epoch 33/100, Train Loss: 0.2443, Val Loss: 0.2560, Val Accuracy: 90.90%, Positive Accuracy: 79.48%, Total Positives: 921, Predicted Positives: 808\n",
      "Epoch 34/100, Train Loss: 0.2598, Val Loss: 0.2515, Val Accuracy: 89.39%, Positive Accuracy: 83.50%, Total Positives: 921, Predicted Positives: 926\n",
      "Epoch 35/100, Train Loss: 0.2421, Val Loss: 0.2464, Val Accuracy: 89.94%, Positive Accuracy: 82.63%, Total Positives: 921, Predicted Positives: 894\n",
      "Epoch 36/100, Train Loss: 0.2455, Val Loss: 0.3261, Val Accuracy: 88.98%, Positive Accuracy: 75.03%, Total Positives: 921, Predicted Positives: 782\n",
      "Epoch 37/100, Train Loss: 0.2449, Val Loss: 0.2481, Val Accuracy: 90.15%, Positive Accuracy: 80.67%, Total Positives: 921, Predicted Positives: 852\n",
      "Epoch 38/100, Train Loss: 0.2408, Val Loss: 0.2485, Val Accuracy: 90.46%, Positive Accuracy: 81.65%, Total Positives: 921, Predicted Positives: 861\n",
      "Epoch 39/100, Train Loss: 0.2310, Val Loss: 0.2466, Val Accuracy: 90.11%, Positive Accuracy: 81.98%, Total Positives: 921, Predicted Positives: 877\n",
      "Epoch 40/100, Train Loss: 0.2344, Val Loss: 0.2537, Val Accuracy: 90.46%, Positive Accuracy: 80.67%, Total Positives: 921, Predicted Positives: 843\n",
      "Epoch 41/100, Train Loss: 0.2331, Val Loss: 0.2496, Val Accuracy: 90.53%, Positive Accuracy: 81.32%, Total Positives: 921, Predicted Positives: 853\n",
      "Epoch 42/100, Train Loss: 0.2402, Val Loss: 0.2460, Val Accuracy: 89.56%, Positive Accuracy: 83.71%, Total Positives: 921, Predicted Positives: 925\n",
      "Epoch 43/100, Train Loss: 0.2269, Val Loss: 0.2471, Val Accuracy: 90.46%, Positive Accuracy: 82.30%, Total Positives: 921, Predicted Positives: 873\n",
      "Epoch 44/100, Train Loss: 0.2278, Val Loss: 0.2510, Val Accuracy: 90.42%, Positive Accuracy: 80.78%, Total Positives: 921, Predicted Positives: 846\n",
      "Epoch 45/100, Train Loss: 0.2316, Val Loss: 0.2446, Val Accuracy: 90.87%, Positive Accuracy: 81.22%, Total Positives: 921, Predicted Positives: 841\n",
      "Epoch 46/100, Train Loss: 0.2246, Val Loss: 0.2451, Val Accuracy: 90.73%, Positive Accuracy: 82.08%, Total Positives: 921, Predicted Positives: 861\n",
      "Epoch 47/100, Train Loss: 0.2253, Val Loss: 0.2472, Val Accuracy: 90.01%, Positive Accuracy: 83.71%, Total Positives: 921, Predicted Positives: 912\n",
      "Epoch 48/100, Train Loss: 0.2239, Val Loss: 0.2467, Val Accuracy: 90.25%, Positive Accuracy: 82.08%, Total Positives: 921, Predicted Positives: 875\n",
      "Epoch 49/100, Train Loss: 0.2265, Val Loss: 0.2518, Val Accuracy: 90.63%, Positive Accuracy: 81.65%, Total Positives: 921, Predicted Positives: 856\n",
      "Epoch 50/100, Train Loss: 0.2213, Val Loss: 0.2407, Val Accuracy: 90.42%, Positive Accuracy: 81.22%, Total Positives: 921, Predicted Positives: 854\n",
      "Epoch 51/100, Train Loss: 0.2234, Val Loss: 0.2426, Val Accuracy: 89.94%, Positive Accuracy: 83.93%, Total Positives: 921, Predicted Positives: 918\n",
      "Epoch 52/100, Train Loss: 0.2228, Val Loss: 0.3051, Val Accuracy: 88.16%, Positive Accuracy: 84.15%, Total Positives: 921, Predicted Positives: 974\n",
      "Epoch 53/100, Train Loss: 0.2418, Val Loss: 0.2528, Val Accuracy: 90.18%, Positive Accuracy: 80.24%, Total Positives: 921, Predicted Positives: 843\n",
      "Epoch 54/100, Train Loss: 0.2233, Val Loss: 0.2567, Val Accuracy: 88.74%, Positive Accuracy: 85.56%, Total Positives: 921, Predicted Positives: 983\n",
      "Epoch 55/100, Train Loss: 0.2270, Val Loss: 0.2598, Val Accuracy: 89.22%, Positive Accuracy: 83.82%, Total Positives: 921, Predicted Positives: 937\n",
      "Epoch 56/100, Train Loss: 0.2205, Val Loss: 0.2468, Val Accuracy: 90.15%, Positive Accuracy: 81.98%, Total Positives: 921, Predicted Positives: 876\n",
      "Epoch 57/100, Train Loss: 0.2163, Val Loss: 0.2497, Val Accuracy: 89.50%, Positive Accuracy: 83.71%, Total Positives: 921, Predicted Positives: 927\n",
      "Epoch 58/100, Train Loss: 0.2185, Val Loss: 0.2397, Val Accuracy: 90.83%, Positive Accuracy: 82.19%, Total Positives: 921, Predicted Positives: 860\n",
      "Epoch 59/100, Train Loss: 0.2093, Val Loss: 0.2353, Val Accuracy: 91.07%, Positive Accuracy: 83.50%, Total Positives: 921, Predicted Positives: 877\n",
      "Epoch 60/100, Train Loss: 0.2135, Val Loss: 0.2367, Val Accuracy: 90.83%, Positive Accuracy: 85.23%, Total Positives: 921, Predicted Positives: 916\n",
      "Epoch 61/100, Train Loss: 0.2094, Val Loss: 0.2368, Val Accuracy: 90.70%, Positive Accuracy: 83.71%, Total Positives: 921, Predicted Positives: 892\n",
      "Epoch 62/100, Train Loss: 0.2172, Val Loss: 0.2413, Val Accuracy: 90.46%, Positive Accuracy: 85.45%, Total Positives: 921, Predicted Positives: 931\n",
      "Epoch 63/100, Train Loss: 0.2089, Val Loss: 0.2400, Val Accuracy: 90.32%, Positive Accuracy: 85.34%, Total Positives: 921, Predicted Positives: 933\n",
      "Epoch 64/100, Train Loss: 0.2064, Val Loss: 0.2346, Val Accuracy: 90.90%, Positive Accuracy: 84.58%, Total Positives: 921, Predicted Positives: 902\n",
      "Epoch 65/100, Train Loss: 0.2045, Val Loss: 0.2356, Val Accuracy: 90.25%, Positive Accuracy: 86.43%, Total Positives: 921, Predicted Positives: 955\n",
      "Epoch 66/100, Train Loss: 0.2041, Val Loss: 0.2355, Val Accuracy: 90.42%, Positive Accuracy: 86.21%, Total Positives: 921, Predicted Positives: 946\n",
      "Epoch 67/100, Train Loss: 0.2010, Val Loss: 0.2342, Val Accuracy: 90.80%, Positive Accuracy: 85.99%, Total Positives: 921, Predicted Positives: 931\n",
      "Epoch 68/100, Train Loss: 0.2001, Val Loss: 0.2364, Val Accuracy: 91.04%, Positive Accuracy: 85.23%, Total Positives: 921, Predicted Positives: 910\n",
      "Epoch 69/100, Train Loss: 0.2014, Val Loss: 0.2383, Val Accuracy: 90.87%, Positive Accuracy: 85.45%, Total Positives: 921, Predicted Positives: 919\n",
      "Epoch 70/100, Train Loss: 0.1977, Val Loss: 0.2543, Val Accuracy: 90.39%, Positive Accuracy: 84.47%, Total Positives: 921, Predicted Positives: 915\n",
      "Epoch 71/100, Train Loss: 0.2064, Val Loss: 0.2349, Val Accuracy: 89.98%, Positive Accuracy: 87.62%, Total Positives: 921, Predicted Positives: 985\n",
      "Epoch 72/100, Train Loss: 0.1997, Val Loss: 0.2364, Val Accuracy: 90.73%, Positive Accuracy: 86.21%, Total Positives: 921, Predicted Positives: 937\n",
      "Epoch 73/100, Train Loss: 0.1954, Val Loss: 0.2375, Val Accuracy: 90.90%, Positive Accuracy: 85.34%, Total Positives: 921, Predicted Positives: 916\n",
      "Epoch 74/100, Train Loss: 0.1932, Val Loss: 0.2384, Val Accuracy: 90.77%, Positive Accuracy: 85.23%, Total Positives: 921, Predicted Positives: 918\n",
      "Epoch 75/100, Train Loss: 0.1915, Val Loss: 0.2358, Val Accuracy: 90.63%, Positive Accuracy: 85.02%, Total Positives: 921, Predicted Positives: 918\n",
      "Epoch 76/100, Train Loss: 0.1895, Val Loss: 0.2396, Val Accuracy: 90.22%, Positive Accuracy: 86.10%, Total Positives: 921, Predicted Positives: 950\n",
      "Epoch 77/100, Train Loss: 0.1879, Val Loss: 0.2358, Val Accuracy: 91.18%, Positive Accuracy: 84.26%, Total Positives: 921, Predicted Positives: 888\n",
      "Epoch 78/100, Train Loss: 0.1872, Val Loss: 0.2404, Val Accuracy: 91.25%, Positive Accuracy: 83.17%, Total Positives: 921, Predicted Positives: 866\n",
      "Epoch 79/100, Train Loss: 0.1862, Val Loss: 0.2416, Val Accuracy: 88.26%, Positive Accuracy: 89.14%, Total Positives: 921, Predicted Positives: 1063\n",
      "Epoch 80/100, Train Loss: 0.1845, Val Loss: 0.2384, Val Accuracy: 89.32%, Positive Accuracy: 86.97%, Total Positives: 921, Predicted Positives: 992\n",
      "Epoch 81/100, Train Loss: 0.1821, Val Loss: 0.2340, Val Accuracy: 89.19%, Positive Accuracy: 88.17%, Total Positives: 921, Predicted Positives: 1018\n",
      "Epoch 82/100, Train Loss: 0.1859, Val Loss: 0.2352, Val Accuracy: 88.71%, Positive Accuracy: 88.82%, Total Positives: 921, Predicted Positives: 1044\n",
      "Epoch 83/100, Train Loss: 0.1800, Val Loss: 0.2356, Val Accuracy: 88.53%, Positive Accuracy: 88.71%, Total Positives: 921, Predicted Positives: 1047\n",
      "Epoch 84/100, Train Loss: 0.1800, Val Loss: 0.2366, Val Accuracy: 90.32%, Positive Accuracy: 86.21%, Total Positives: 921, Predicted Positives: 949\n",
      "Epoch 85/100, Train Loss: 0.1791, Val Loss: 0.2310, Val Accuracy: 89.74%, Positive Accuracy: 87.19%, Total Positives: 921, Predicted Positives: 984\n",
      "Epoch 86/100, Train Loss: 0.1778, Val Loss: 0.2329, Val Accuracy: 89.63%, Positive Accuracy: 88.27%, Total Positives: 921, Predicted Positives: 1007\n",
      "Epoch 87/100, Train Loss: 0.1759, Val Loss: 0.2378, Val Accuracy: 89.74%, Positive Accuracy: 87.73%, Total Positives: 921, Predicted Positives: 994\n",
      "Epoch 88/100, Train Loss: 0.1772, Val Loss: 0.2382, Val Accuracy: 89.39%, Positive Accuracy: 88.60%, Total Positives: 921, Predicted Positives: 1020\n",
      "Epoch 89/100, Train Loss: 0.1756, Val Loss: 0.2374, Val Accuracy: 88.98%, Positive Accuracy: 88.38%, Total Positives: 921, Predicted Positives: 1028\n",
      "Epoch 90/100, Train Loss: 0.1748, Val Loss: 0.2485, Val Accuracy: 90.28%, Positive Accuracy: 85.45%, Total Positives: 921, Predicted Positives: 936\n",
      "Epoch 91/100, Train Loss: 0.1716, Val Loss: 0.2384, Val Accuracy: 88.95%, Positive Accuracy: 87.51%, Total Positives: 921, Predicted Positives: 1013\n",
      "Epoch 92/100, Train Loss: 0.1712, Val Loss: 0.2373, Val Accuracy: 87.92%, Positive Accuracy: 89.14%, Total Positives: 921, Predicted Positives: 1073\n",
      "Epoch 93/100, Train Loss: 0.1711, Val Loss: 0.2376, Val Accuracy: 89.05%, Positive Accuracy: 89.03%, Total Positives: 921, Predicted Positives: 1038\n",
      "Epoch 94/100, Train Loss: 0.1715, Val Loss: 0.2404, Val Accuracy: 89.29%, Positive Accuracy: 88.17%, Total Positives: 921, Predicted Positives: 1015\n",
      "Epoch 95/100, Train Loss: 0.1696, Val Loss: 0.2395, Val Accuracy: 89.98%, Positive Accuracy: 86.21%, Total Positives: 921, Predicted Positives: 959\n",
      "Epoch 96/100, Train Loss: 0.1675, Val Loss: 0.2422, Val Accuracy: 89.26%, Positive Accuracy: 88.17%, Total Positives: 921, Predicted Positives: 1016\n",
      "Epoch 97/100, Train Loss: 0.1695, Val Loss: 0.2402, Val Accuracy: 89.26%, Positive Accuracy: 88.17%, Total Positives: 921, Predicted Positives: 1016\n",
      "Epoch 98/100, Train Loss: 0.1651, Val Loss: 0.2355, Val Accuracy: 89.77%, Positive Accuracy: 87.19%, Total Positives: 921, Predicted Positives: 983\n",
      "Epoch 99/100, Train Loss: 0.1622, Val Loss: 0.2380, Val Accuracy: 89.12%, Positive Accuracy: 87.62%, Total Positives: 921, Predicted Positives: 1010\n",
      "Epoch 100/100, Train Loss: 0.1639, Val Loss: 0.2428, Val Accuracy: 87.06%, Positive Accuracy: 90.88%, Total Positives: 921, Predicted Positives: 1130\n",
      "Test Loss: 0.2347, Test Accuracy: 88.19%, Test Positive Accuracy: 90.99%, Test Total Positives: 932, Test Predicted Positives: 1108\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, args):\n",
    "    \"\"\"\n",
    "    预测测试用例触发 bug 的概率，并按照概率值降序排序。\n",
    "\n",
    "    参数:\n",
    "        model: 已经训练好的模型。\n",
    "        data_loader: DataLoader 实例，用于加载测试数据集。\n",
    "        args: 参数配置。\n",
    "\n",
    "    返回:\n",
    "        bug_indices: 按照触发 bug 概率降序排序的测试用例索引序列。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_graphs = batch.batch_graph()\n",
    "            encoded_graph_infos = [datapoint.graph_info for datapoint in batch]\n",
    "            output = model(batch_graphs, encoded_graph_infos)\n",
    "            predictions.extend(output.squeeze(1).tolist())\n",
    "\n",
    "    # 将预测结果和对应的测试用例组合成元组列表\n",
    "    bug_probabilities = list(enumerate(predictions))\n",
    "\n",
    "    # 按照触发 bug 概率降序排序\n",
    "    bug_probabilities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 提取排序后的测试用例索引\n",
    "    bug_indices = [idx for idx, _ in bug_probabilities]\n",
    "\n",
    "    return bug_indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "json_filename = 'demo_test_data_info.json'\n",
    "with open(json_filename, 'r') as jsonfile:\n",
    "    demo_test_data_info = json.load(jsonfile)\n",
    "json_filename = 'demo_test_time_info.json'\n",
    "with open(json_filename, 'r') as jsonfile:\n",
    "    demo_test_time_info = json.load(jsonfile)\n",
    "test_node_info_set = [item['node_info'] for item in demo_test_data_info]\n",
    "test_edge_info_set = [item['edge_info'] for item in demo_test_data_info]\n",
    "test_graph_info_set = [item['graph_info'] for item in demo_test_data_info]\n",
    "test_result_set = [item['result'] for item in demo_test_data_info]\n",
    "\n",
    "demo_test_data = CompGraphDataset([\n",
    "    CompGraphDatapoint(\n",
    "        node_info = test_node_info_set[i],\n",
    "        edge_info = test_edge_info_set[i],\n",
    "        graph_info = test_graph_info_set[i],\n",
    "        result = test_result_set[i],\n",
    "        op_vocab=op_vocab\n",
    "    ) for i in range(len(test_node_info_set))\n",
    "])\n",
    "demo_test_data_loader = CompGraphDataLoader(\n",
    "    dataset = demo_test_data,\n",
    "    batch_size = args.batch_size,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    seed = args.seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1627, Test Accuracy: 88.50%, Test Positive Accuracy: 80.00%, Test Total Positives: 10, Test Predicted Positives: 29\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_positive_accuracy, test_total_positives, test_predicted_positives = evaluate(model, demo_test_data_loader, args)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, '\n",
    "          f'Test Positive Accuracy: {test_positive_accuracy:.2f}%, Test Total Positives: {test_total_positives}, Test Predicted Positives: {test_predicted_positives}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 30,\n",
       " 67,\n",
       " 171,\n",
       " 194,\n",
       " 80,\n",
       " 64,\n",
       " 38,\n",
       " 138,\n",
       " 104,\n",
       " 121,\n",
       " 29,\n",
       " 50,\n",
       " 102,\n",
       " 9,\n",
       " 99,\n",
       " 3,\n",
       " 178,\n",
       " 174,\n",
       " 48,\n",
       " 163,\n",
       " 134,\n",
       " 189,\n",
       " 2,\n",
       " 122,\n",
       " 24,\n",
       " 172,\n",
       " 129,\n",
       " 83,\n",
       " 192,\n",
       " 88,\n",
       " 141,\n",
       " 151,\n",
       " 187,\n",
       " 51,\n",
       " 12,\n",
       " 146,\n",
       " 23,\n",
       " 5,\n",
       " 25,\n",
       " 110,\n",
       " 32,\n",
       " 109,\n",
       " 116,\n",
       " 136,\n",
       " 93,\n",
       " 42,\n",
       " 108,\n",
       " 143,\n",
       " 191,\n",
       " 95,\n",
       " 77,\n",
       " 16,\n",
       " 87,\n",
       " 183,\n",
       " 195,\n",
       " 137,\n",
       " 97,\n",
       " 79,\n",
       " 158,\n",
       " 177,\n",
       " 6,\n",
       " 11,\n",
       " 35,\n",
       " 150,\n",
       " 186,\n",
       " 0,\n",
       " 147,\n",
       " 170,\n",
       " 81,\n",
       " 53,\n",
       " 113,\n",
       " 114,\n",
       " 65,\n",
       " 166,\n",
       " 188,\n",
       " 176,\n",
       " 45,\n",
       " 173,\n",
       " 18,\n",
       " 167,\n",
       " 58,\n",
       " 63,\n",
       " 84,\n",
       " 103,\n",
       " 26,\n",
       " 165,\n",
       " 181,\n",
       " 69,\n",
       " 43,\n",
       " 27,\n",
       " 17,\n",
       " 127,\n",
       " 115,\n",
       " 82,\n",
       " 131,\n",
       " 61,\n",
       " 10,\n",
       " 168,\n",
       " 41,\n",
       " 120,\n",
       " 179,\n",
       " 49,\n",
       " 21,\n",
       " 28,\n",
       " 190,\n",
       " 54,\n",
       " 197,\n",
       " 36,\n",
       " 56,\n",
       " 164,\n",
       " 148,\n",
       " 33,\n",
       " 22,\n",
       " 72,\n",
       " 92,\n",
       " 74,\n",
       " 162,\n",
       " 94,\n",
       " 86,\n",
       " 85,\n",
       " 135,\n",
       " 198,\n",
       " 139,\n",
       " 70,\n",
       " 75,\n",
       " 111,\n",
       " 98,\n",
       " 66,\n",
       " 132,\n",
       " 90,\n",
       " 91,\n",
       " 169,\n",
       " 184,\n",
       " 13,\n",
       " 71,\n",
       " 57,\n",
       " 185,\n",
       " 123,\n",
       " 130,\n",
       " 153,\n",
       " 19,\n",
       " 20,\n",
       " 68,\n",
       " 152,\n",
       " 157,\n",
       " 161,\n",
       " 156,\n",
       " 124,\n",
       " 59,\n",
       " 101,\n",
       " 78,\n",
       " 1,\n",
       " 7,\n",
       " 133,\n",
       " 199,\n",
       " 34,\n",
       " 128,\n",
       " 155,\n",
       " 62,\n",
       " 60,\n",
       " 140,\n",
       " 112,\n",
       " 8,\n",
       " 96,\n",
       " 107,\n",
       " 37,\n",
       " 73,\n",
       " 118,\n",
       " 52,\n",
       " 119,\n",
       " 154,\n",
       " 14,\n",
       " 180,\n",
       " 159,\n",
       " 89,\n",
       " 106,\n",
       " 117,\n",
       " 15,\n",
       " 125,\n",
       " 100,\n",
       " 4,\n",
       " 144,\n",
       " 55,\n",
       " 126,\n",
       " 31,\n",
       " 76,\n",
       " 105,\n",
       " 149,\n",
       " 40,\n",
       " 145,\n",
       " 175,\n",
       " 193,\n",
       " 39,\n",
       " 142,\n",
       " 182,\n",
       " 160,\n",
       " 196,\n",
       " 44,\n",
       " 47]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_indices = predict(model, demo_test_data_loader, args)\n",
    "bug_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 30, 46, 67, 80, 108, 138, 163, 171, 187]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_bug_indices = []\n",
    "for i in range(len(test_result_set)):\n",
    "    if test_result_set[i]!=0:\n",
    "        real_bug_indices.append(i)\n",
    "real_bug_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
